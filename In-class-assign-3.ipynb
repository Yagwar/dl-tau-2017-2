{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class Assignment 3\n",
    "## [COSC 7336 Advanced Natural Language](https://fagonzalezo.github.io/dl-tau-2017-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorship Attribution with CNNs\n",
    "\n",
    "The goal is to build a model to identify the author of a text. We will use the CCAT Reuters 50-50 dataset (https://archive.ics.uci.edu/ml/datasets/Reuter_50_50). The training corpus consists of 2,500 texts (50 per author) and the test corpus includes other 2,500 texts (50 per author) non-overlapping with the training texts. First you have to download the file `c50.zip` from the UCI repository. If your are working on an Azure you can use `wget` or the following Python code that downloads the file from `URL` and saves it locally under `file_name`::\n",
    "\n",
    "```python\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('URL', \"filename\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After you have the file, the following code will unzip it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the c50.zip file\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile(\"./C50.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes the dataset files and loads the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 files loaded from ./C50/C50train\n",
      "2500 files loaded from ./C50/C50test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def get_texts_from_catdir(cat_dir):\n",
    "    texts = []\n",
    "    TRAIN_DIR = cat_dir#\"./c50/train\"\n",
    "    category_index = {}\n",
    "    categories = []\n",
    "    for category_name in sorted(os.listdir(TRAIN_DIR)):\n",
    "        category_id = len(category_index)\n",
    "        category_index[category_name] = category_id\n",
    "        #print(category_name)\n",
    "        category_path = os.path.join(TRAIN_DIR, category_name)\n",
    "        for f_name in sorted(os.listdir(category_path)):\n",
    "            f_path = os.path.join(category_path, f_name)\n",
    "            #print(f_name)\n",
    "            #print(f_path)\n",
    "            f = open(f_path, \"r\")\n",
    "            texts += [f.read()]                \n",
    "            f.close()\n",
    "            categories += [category_id]\n",
    "    print(\"%d files loaded from %s\" % (len(texts), cat_dir))\n",
    "    return texts, categories, category_index\n",
    "\n",
    "# Load the RAW text and Category labels\n",
    "tr_txt, tr_y, tr_y_ind = get_texts_from_catdir(\"./C50/C50train\")\n",
    "te_txt, te_y, te_y_ind = get_texts_from_catdir(\"./C50/C50test\")\n",
    "#print(tr_y)\n",
    "#print(tr_y_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we extract the words from the text and represent is sample as a sequence of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31090 unique tokens.\n",
      "Frequency of \"the\" is 71425\n",
      "WORD-SEQUENCE OF A TEXT:\n",
      "['the', 'internet', 'may', 'be', 'overflowing']\n",
      "TEXT CONVERTED TO A SEQUENCE OF IDs: \n",
      "[1, 169, 130, 14, 13]\n"
     ]
    }
   ],
   "source": [
    "# Build Tokenizer and Vocabulary\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(tr_txt)\n",
    "# Dictionary of the WHOLE extracted vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# Frequencies of words\n",
    "word_counts=tokenizer.word_counts\n",
    "print(\"Frequency of \\\"%s\\\" is %s\" % (\"the\", word_counts[\"the\"]))\n",
    "\n",
    "print(\"WORD-SEQUENCE OF A TEXT:\")\n",
    "sample_text = keras.preprocessing.text.text_to_word_sequence(tr_txt[0])\n",
    "print(sample_text[:5])\n",
    "\n",
    "print(\"TEXT CONVERTED TO A SEQUENCE OF IDs: \")\n",
    "X_tr_seq = tokenizer.texts_to_sequences(tr_txt)\n",
    "X_te_seq = tokenizer.texts_to_sequences(te_txt)\n",
    "print(X_tr_seq[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Now, it's time for you to work. \n",
    "\n",
    "### 1. Small dataset experiment\n",
    "\n",
    "Adapt the code in https://github.com/fagonzalezo/dl-tau-2017-2/blob/master/Handout-CNN-sentence-classification.ipynb to work with the the authorship dataset that we just processed. We will train the CNN-rand model (no word2vec vectors).\n",
    "\n",
    "We need to shuffle the train dataset, since it is sorted by author. Also, for the first experiment we will take only 500 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Classes: 50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x_train, y_train = shuffle(X_tr_seq, tr_y, random_state=0)\n",
    "\n",
    "\n",
    "x_test = X_te_seq\n",
    "y_test = te_y\n",
    "\n",
    "x_train = x_train[:500]\n",
    "y_train = y_train[:500]\n",
    "\n",
    "num_classes = len(set(y_train))\n",
    "print(\"Num Classes: \" + str(num_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Small dataset with word2vec\n",
    "\n",
    "Download the word2vec pretrained model from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing. If you are using an Azure VM you may need to use the following utility to download files from Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    '''\n",
    "    id: file id (take it from sharable link)\n",
    "    destination: destination file in your disk\n",
    "    '''\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Full dataset experiment\n",
    "\n",
    "Repeat the experimen using now the whole dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
