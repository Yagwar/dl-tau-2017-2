{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "# COSC 7336 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The items 1 and 2 of the assignment must be completed in this notebook. Before submitting, check that the notebook executes properly and that the corresponding outputs render appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Character-based language model practical\n",
    "Use a pretrained model character-based model such as the one discussed in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The most probable next word.\n",
    "\n",
    "Build a function that calculates a word that, with high probability, may follow a given one. Formally, given a word $w_i$ it must be calculated $w_{i+1}$ such that: \n",
    "\n",
    "$$ w_{i+1} = \\arg \\max_{w_{i+1}} P(w_{i+1}|w_i) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_word(word):\n",
    "    # your code here\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly explain your strategy. Test the function with different examples. Discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The hangman\n",
    "\n",
    "Design a function able to find the missing characters from a word. The function must work as follows:\n",
    "\n",
    "```\n",
    ">>> hangman(\"pe_p_e\")\n",
    "'people'\n",
    "\n",
    ">>> hangman(\"phi__sop_y\")\n",
    "'philosophy'\n",
    "\n",
    ">>> hangman(\"si_nif_c_nc_\")\n",
    "'significance'\n",
    "\n",
    ">>> hangman(\"kn__l_d_e\")\n",
    "'knowledge'\n",
    "\n",
    ">>> hangman(\"inte_r_ga_i_n\")\n",
    "'interrogation'\n",
    "```\n",
    "\n",
    "The function must be able to deal with up to 4 unknowns in arbitrary length words. The function must work in a reasonable time (max 1 minute in a laptop). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hangman(word):\n",
    "    ### your code here\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly explain your strategy. Test the function with different examples. Discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Spell correction with a neural language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to load the text news into: i) list of strings and ii) list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 files loaded.\n",
      "['scientists', 'witness', 'huge', 'cosmic', 'crash', 'find', 'origins', 'of', 'gold', 'even']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# simple extraction of words\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# siple loading of the documents\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def get_texts_from_catdir(cat_dir):\n",
    "    texts = []\n",
    "    TARGET_DIR = cat_dir # \"./target\"\n",
    "    for f_name in sorted(os.listdir(TARGET_DIR)):\n",
    "        f_path = os.path.join(TARGET_DIR, f_name)\n",
    "        #print(f_name)\n",
    "        #print(f_path)\n",
    "        f = open(f_path, \"r\")\n",
    "        texts += [f.read()]                \n",
    "        f.close()\n",
    "    print(\"%d files loaded.\" % len(texts))\n",
    "    return texts\n",
    "\n",
    "# Load the RAW text \n",
    "target_txt = get_texts_from_catdir(\"./target\")\n",
    "\n",
    "# Print first 10 words in document0\n",
    "print(words(target_txt[0])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Build a basic spell corrector and generate candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the supporting files for this question from [here](https://github.com/fagonzalezo/dl-tau-2017-2/blob/master/assign2_q2_files.zip). Uncompressed the zip folder and move your notebook file to that folder. You are given a set of 10 news in the \"target\" directory located in the folder. Follow the tutorial in [this guide](http://norvig.com/spell-correct.html) to create a basic spell corrector and complete this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Merge WORDS[word] using DIC_WORDS_IN_NEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the information in DIC_WORDS_IN_NEWS to merge the dictionary WORDS[word] and their counts. Make sure that if a word in DIC_WORDS_IN_NEWS already exists in WORD, the resulting counts are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('WORDS_IN_NEWS.txt', 'r') as infile:\n",
    "    WORDS_IN_NEWS = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dic(words_dic, words_in_news_dic):\n",
    "    '''\n",
    "    words_dic: for example WORDS\n",
    "    words_in_news_dic: for example WORDS_IN_NEWS\n",
    "    Returns: Merged dictionary merged_word_dic\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return merged_word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDS = merge_dic(WORDS, WORDS_IN_NEWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Detect misspelled words in the NEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Create a function that given the words in a text, returns a list containing the tuples of each misspelled word and its list of candidate words. The function is expected to find the out-of-vocabulary words and candidate replacements of each one. Feel free to reuse code from the Norvig's blog above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def misspelled_and_candidates(target_words):\n",
    "    '''\n",
    "    target_words: a text with possible misspellings represented as a list of words\n",
    "    Returns: list of tuples of the form [(\"misspelled-word-1\", [\"candidate-word-1_1\", \"candidate-word-1_2\"]), ...]\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return misspelled_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example call to print the output. Target_txt[0] is the raw data from file 0.\n",
    "print(misspelled_and_candidates(words(target_txt[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print misspelled words and candidates for each document in target_txt list\n",
    "# Your code goes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build a new char LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a character language model using the code in the [LSTM handout](https://github.com/fagonzalezo/dl-tau-2017-2/blob/master/Handout-LSTM-language-model.ipynb), but train the model using this [file](http://norvig.com/big.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE FOR BUILDING AND TRAINING A LSTM GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Full spell correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your trained LSTM model to build a function that inspects and corrects a text (input as a list of words). This function has to use the previously defined \"misspelled_and_candidates\", but let the LSTM choose the best candidate and apply the correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spell_correction(input_text):\n",
    "    '''\n",
    "    input_text: a text with possible misspellings represented as a list of words\n",
    "    Returns: the text with corrected misspellings\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the correction to each target text\n",
    "corrected_txt = []\n",
    "for t_txt in target_txt:\n",
    "    corrected_txt += [spell_correction(words(t_txt))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate your spell correction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_errors(corrected_text, reference_text):\n",
    "    if len(corrected_text) != len(reference_text):\n",
    "        print (\"The length of the lists does not match!\")\n",
    "        return\n",
    "    \n",
    "    errors = 0\n",
    "    for corrected_word, reference_word in zip(corrected_text, reference_text):\n",
    "        if corrected_word != reference_word:\n",
    "            errors += 1\n",
    "            \n",
    "    return errors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 files loaded.\n",
      "Total errors before correction: 11\n",
      "Total errors after correction: 0\n"
     ]
    }
   ],
   "source": [
    "# Compare errors before and after correction\n",
    "# Ideally errors after correction should be 0, but they depend on the LSTM model. We will evaluate this part \n",
    "# with a held out set of text. \n",
    "\n",
    "golden_txt = get_texts_from_catdir(\"./golden\")\n",
    "errors_before=0\n",
    "errors_after=0\n",
    "for t_txt, g_txt, c_txt in zip(target_txt, golden_txt, corrected_txt):\n",
    "    errors_before += number_of_errors(words(t_txt), words(g_txt))\n",
    "    errors_after += number_of_errors(words(c_txt), words(g_txt))\n",
    "print(\"Total errors before correction:\", errors_before)\n",
    "print(\"Total errors after correction:\", errors_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
