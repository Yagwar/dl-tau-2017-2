{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence neural models for text translation handout\n",
    "## [COSC 7336 Advanced Natural Language Processing](https://fagonzalezo.github.io/dl-tau-2017-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this handout we build a Neural Machine Translation model based Long-Short Term Memory (LSTM) for sequence to sequence prediction using Keras. This handout is based on the code here (https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py), which is discussed in this [blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html).\n",
    "\n",
    "The model uses two LSTMs, one for encoding a sentence in one language and the other one to generate (decode) a corresponding sentence in the target language. The model is depicted in the following figure:\n",
    "\n",
    "![encoder-decoder](https://camo.githubusercontent.com/80fdbf3e4aa1ad2c9c0d8bef7c392cbe862ee650/68747470733a2f2f65736369656e636567726f75702e66696c65732e776f726470726573732e636f6d2f323031362f30332f736571327365712e6a70673f773d363235)\n",
    "\n",
    "However, here we will implement character-based model instead of a word-based model. So it looks more like this model:\n",
    "\n",
    "![encoder-decoder](https://www.tensorflow.org/images/basic_seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a paired corpus of English-Spanish sentences available [here](http://www.manythings.org/anki/spa-eng.zip). The following code reads the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'spa-eng/spa.txt' # Path to the data txt file on disk.\n",
    "num_samples = 100000  # Number of samples to train on.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "lines = open(data_path).read().split('\\n')\n",
    "#for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "for line in lines[:num_samples]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    if len(target_text) > 100 or len(input_text) > 100:\n",
    "        break\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some statistics from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "Number of unique input tokens: 86\n",
      "Number of unique output tokens: 106\n",
      "Max sequence length for inputs: 46\n",
      "Max sequence length for outputs: 85\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some examples of the pairs in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go. \tVe.\n",
      "\n",
      "I'm faster. \tYo soy más rápido.\n",
      "\n",
      "We help Tom. \tNosotros ayudamos a Tom.\n",
      "\n",
      "Who is there? \t¿Quién está ahí?\n",
      "\n",
      "It works well. \tFunciona bien.\n",
      "\n",
      "Here comes Tom. \tAquí viene Tom.\n",
      "\n",
      "Tom can't walk. \tTom no puede andar.\n",
      "\n",
      "I know it's hot. \tSé que está caliente.\n",
      "\n",
      "This is for you. \tEsto es para ti.\n",
      "\n",
      "He became famous. \tÉl se hizo famoso.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10000, 1000):\n",
    "    print(input_texts[i],target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Characters will be encoded using a one-hot-representation. We also build dictionaries to map from characters to indices and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_data shape: (100000, 46, 86)\n",
      "decoder_input_data shape: (100000, 85, 106)\n"
     ]
    }
   ],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "print(\"encoder_input_data shape:\", encoder_input_data.shape)\n",
    "print(\"decoder_input_data shape:\", decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As depicted in the previous figures, the model uses two LSTM layers. The first one is the encoder. The encoder process the input sentence in the source language, character by character. The output of the layer is not important. Instead, we use it internal state represented by the internal variables $h$ and $c$. In this implementation, we will use the last state, $h_n$ and $c_n$, the state of the LSTM units after processing the $n$ characters of the input. The following code define the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the decoder is to generate the output sentence in the target language. It uses the internal state of the encoder as initial state for generation. It works as a character-based language model. At each step it generates an output character $y_n$ using as input the previous character $y_{n-1}$. During training, we will use the real targets as input to the decoder instead of the characters generated by the LSTM. This is called *teacher forcing*. So, the decoder also receive the target values as inputs (`decoder_input_data`), but shifted by one time step. The initial state of the decoder is set to the last state of the encoder. The output of the decoder at each time step is a distribution over all the possible output characters (`num_decoder_tokens`), this is performed by a dense layer with a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is defined with inputs corresponding to both the encoder and decoder inputs and the output corresponding to the output of the decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_9 (InputLayer)             (None, None, 86)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, None, 106)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    [(None, 256), (None,  351232      input_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    [(None, None, 256), ( 371712      input_10[0][0]                   \n",
      "                                                                   lstm_5[0][1]                     \n",
      "                                                                   lstm_5[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, None, 106)     27242       lstm_6[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 750,186\n",
      "Trainable params: 750,186\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready for training the model. Since the output of the decoder is softmax we use a categorical cross entropy loss. The best model so far is stored using a `ModelCheckpoint` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.4182 - val_loss: 0.6882\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.3813 - val_loss: 0.6485\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.3582 - val_loss: 0.6237\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.3418 - val_loss: 0.6063\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.3295 - val_loss: 0.5987\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.3197 - val_loss: 0.5859\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.3117 - val_loss: 0.5770\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.3048 - val_loss: 0.5737\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2989 - val_loss: 0.5705\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 480s 5ms/step - loss: 0.2935 - val_loss: 0.5647\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2887 - val_loss: 0.5637\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 477s 5ms/step - loss: 0.2843 - val_loss: 0.5582\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2804 - val_loss: 0.5569\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2768 - val_loss: 0.5553\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2736 - val_loss: 0.5570\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2706 - val_loss: 0.5549\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2678 - val_loss: 0.5526\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 477s 5ms/step - loss: 0.2652 - val_loss: 0.5549\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 480s 5ms/step - loss: 0.2628 - val_loss: 0.5511\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2605 - val_loss: 0.5514\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2585 - val_loss: 0.5506\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2563 - val_loss: 0.5514\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2545 - val_loss: 0.5520\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2526 - val_loss: 0.5540\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2509 - val_loss: 0.5545\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2492 - val_loss: 0.5533\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2477 - val_loss: 0.5546\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 479s 5ms/step - loss: 0.2463 - val_loss: 0.5563\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2447 - val_loss: 0.5577\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 478s 5ms/step - loss: 0.2433 - val_loss: 0.5581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b0a055c50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "\n",
    "# Run training\n",
    "callback = ModelCheckpoint('s2s_weights.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                           save_best_only=True, save_weights_only=True)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.05,\n",
    "          callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell load a pretrained model that was trained with 100000 samples\n",
    "# The values of the following variables have to be set before defining the model:\n",
    "# Number of unique input tokens: 86\n",
    "# Number of unique output tokens: 106\n",
    "# Max sequence length for inputs: 46\n",
    "# Max sequence length for outputs: 85\n",
    "\n",
    "model.load_weights('s2s_weights.21-0.55.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a different model for prediction taking into accournt that during prediction we don't receive inputs for the decoder. Instead we use the output of the decoder from the previous time step. Also we define two separate models for the decoder and the encoder so that we can call them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None, 86)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                [(None, 256), (None, 256) 351232    \n",
      "=================================================================\n",
      "Total params: 351,232\n",
      "Trainable params: 351,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, None, 106)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_7 (InputLayer)             (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    [(None, None, 256), ( 371712      input_4[0][0]                    \n",
      "                                                                   input_7[0][0]                    \n",
      "                                                                   input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, None, 106)     27242       lstm_4[2][0]                     \n",
      "====================================================================================================\n",
      "Total params: 398,954\n",
      "Trainable params: 398,954\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "encoder_model.summary()\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the models for generating translations for input sentences. First we use a a straightforward approach that selects the character with maximum probability at each step of the decoder execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that takes as input a text and encode it using the one-hot-representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder_input(input_text):\n",
    "    result = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')    \n",
    "    for t, char in enumerate(input_text):\n",
    "        result[0, t, input_token_index[char]] = 1.\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for testing our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No sabía que estoy cansado.\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(one_hot_encoder_input(\"I didn't know you.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this straightforward generation strategy is that it is greedy, so it does not necessarly generate the sequence with the highest probability. Generating the maximum-probability sequence is a hard computational problem, however there are strategies that help to deal with the combinatorial explosion. One of the most popular ones is [Beam Search](https://en.wikipedia.org/wiki/Beam_search). The idea is to generate a set of candidate characters with high probability at each step, instead of genearating only the one with the maximum probability, and look ahead a number of stesp.  \n",
    "\n",
    "The following image illustrates the overall idea: \n",
    "\n",
    "![beam search](https://cloud.githubusercontent.com/assets/1135354/12116947/9e9898d2-b3bd-11e5-9f23-b2c8ba03ab52.jpg)\n",
    "\n",
    "At each stept several candidate characters are considered, the look ahead allows to explore different combinations and choose the one with the highest probability:\n",
    "\n",
    "\n",
    "The following code implements beam search. The parameters of the function are as follows:\n",
    "\n",
    "* `states_value`: the value of the internal states of the decoder from the previous step.\n",
    "* `target_idx`: the index of the las character generated.\n",
    "* `lahead`: how many steps to look ahead.\n",
    "* `nbeams`: how many different candidate characters (beams) to consider at each step.\n",
    "* `decrement`: the number of beams can be decremented at each step by this parameter\n",
    "* `min_nbeams`: the minimum number of beams, if the number of beams is equal to min_nbeams it is not further decreased.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(states_value, target_idx, lahead=3, \n",
    "                nbeams = 2, min_nbeams = 1, decrement = 0):\n",
    "    nbeams = max(nbeams, min_nbeams)\n",
    "    if reverse_target_char_index[target_idx] == '\\n' or lahead < 1:\n",
    "        return ([], 0, 0)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_idx] = 1.\n",
    "    output_tokens, h, c = decoder_model.predict(\n",
    "        [target_seq] + states_value)\n",
    "    states_value = [h, c]\n",
    "    # Sample a token\n",
    "    candidates = [(output_tokens[0, -1, i], i) for i in range(output_tokens.shape[2])]\n",
    "    candidates.sort()\n",
    "    best_prob = float('-inf')\n",
    "    best_idx = 0\n",
    "    for prob, idx in candidates[-nbeams:]:\n",
    "        _, _, cand_log_prob = beam_search(states_value, idx, lahead - 1,\n",
    "                                                           nbeams - decrement, min_nbeams, decrement)\n",
    "        cand_log_prob += np.log(prob)\n",
    "        if cand_log_prob > best_prob:\n",
    "            best_prob = cand_log_prob\n",
    "            best_idx = idx\n",
    "    return (states_value, best_idx, best_prob)\n",
    "\n",
    "def decode_sequence_beam(input_seq, lahead=3, nbeams = 2, min_nbeams = 1, decrement = 0):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    decoded_sentence = ''\n",
    "    best_idx = target_token_index['\\t']\n",
    "    for i in range(max_decoder_seq_length):\n",
    "        states_value, best_idx, best_prob = beam_search(states_value, best_idx , \n",
    "                                                        nbeams = nbeams, \n",
    "                                                        min_nbeams = min_nbeams, \n",
    "                                                        decrement = decrement)\n",
    "        sampled_char = reverse_target_char_index[best_idx]\n",
    "        if sampled_char == '\\n':\n",
    "            break\n",
    "        decoded_sentence += sampled_char\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No sabía que estoy.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence_beam(one_hot_encoder_input(\"I didn't know you.\"), lahead=4, nbeams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
